---
title: "Homework 3"
author: "Allie Caughman"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE, message = FALSE,warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)

library(tidyverse)
library(tidymodels)
library(discrim)
library(MASS)
library(klaR)
library(naniar)
library(visdat)
library(corrplot)

set.seed(42)
```

## Classification

For this assignment, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

```{r}
titanic = read_csv(here::here("data", "titanic.csv")) %>% #read data
  mutate(survived = as.factor(survived)) %>%#mutate into factors
  mutate(pclass = as.factor(pclass))

titanic$survived = factor(titanic$survived, levels = c("Yes", "No")) #reorder factor survived
```


Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. Take a look at the training data and note any potential issues, such as missing data.

```{r}
data_split = initial_split(titanic, prop = .75, strata = survived) #split data stratified by survived

titanic_train = training(data_split) #get training data
titanic_test = testing(data_split) #get testing data
```

```{r}
#visualize NAs
vis_dat(titanic_train) + labs(title = "Training Data")
vis_dat(titanic_test) + labs(title = "Testing Data")
```

**Answer:** There is lots of missing data in cabin, as well as in age. This could reduce the amount of data if used in the model.

Why is it a good idea to use stratified sampling for this data?

**Answer:** Stratified sampling ensures that a representative amount of each level of the variable is included in the training and testing dataset. Without straifying, random chance may not split data well.

### Question 2

Using the **training** data set, explore/describe the distribution of the outcome variable `survived`.

```{r}
ggplot(titanic_train, aes(survived)) +
  geom_bar() + #create a bar graph of counts of each factor level
  theme_bw() +#update theme
  labs(x = "Survived?", y = "Count") #change axis labels
```

**Answer:** More people died in the titanic than survived. 

### Question 3

Using the **training** data set, create a correlation matrix of all continuous variables. Create a visualization of the matrix, and describe any patterns you see. Are any predictors correlated with each other? Which ones, and in which direction?

```{r}
titanic_cor = titanic_train %>% 
  select_if(is.numeric) %>% #select numerics
  cor() #get correlations

corrplot(titanic_cor, method = "number", type="lower") #plot correlations
```

**Answer:** The highest correlation is between the number of siblings/spouses and the number of parents or children also on board the titanic. This is a moderate positive correlation. Then number of parents or children/number of siblings or spouses and fare are also slightly positively correlated. 

### Question 4

Using the **training** data, create a recipe predicting the outcome variable `survived`. Include the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to **dummy** encode categorical predictors. Finally, include interactions between:

-   Sex and passenger fare, and
-   Age and passenger fare.

```{r}
titanic_recipe = recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%  #create recipe
  step_impute_linear(age) %>% #impute values for age
  step_dummy(all_nominal_predictors()) %>% #create dummy variables
  step_interact(terms = ~sex:fare) %>% 
  step_interact(terms = ~age:fare)# create interaction terms
```


### Question 5

Specify a **logistic regression** model for classification using the `"glm"` engine. Then create a workflow. Add your model and the appropriate recipe. Finally, use `fit()` to apply your workflow to the **training** data.

```{r}
glm_model = logistic_reg() %>% 
  set_engine("glm") #set logistic regression using glm engine
```

```{r}
titanic_wflow1 = workflow() %>% #create workflow
  add_model(glm_model) %>% #add model
  add_recipe(titanic_recipe) #add recipe 

glm_fit = fit(titanic_wflow1, titanic_train) #fit model to training data
```

### Question 6

**Repeat Question 5**, but this time specify a linear discriminant analysis model for classification using the `"MASS"` engine.

```{r}
lda_model = discrim_linear() %>% 
  set_engine("MASS") %>%  #set linear discriminant analysis using MASS engine
  set_mode("classification")
```

```{r}
titanic_wflow2 = workflow() %>% #create workflow
  add_model(lda_model) %>% #add model
  add_recipe(titanic_recipe) #add recipe 

lda_fit = fit(titanic_wflow2, titanic_train) #fit model to training data
```


### Question 7

**Repeat Question 5**, but this time specify a quadratic discriminant analysis model for classification using the `"MASS"` engine.


```{r}
qda_model = discrim_quad() %>% 
  set_engine("MASS") %>%  #set quadratic discriminant analysis using MASS engine
  set_mode("classification")
```

```{r}
titanic_wflow3 = workflow() %>% #create workflow
  add_model(qda_model) %>% #add model
  add_recipe(titanic_recipe) #add recipe 

qda_fit = fit(titanic_wflow3, titanic_train) #fit model to training data
```


### Question 8

**Repeat Question 5**, but this time specify a naive Bayes model for classification using the `"klaR"` engine. Set the `usekernel` argument to `FALSE`.


```{r}
nb_model = naive_Bayes() %>% 
  set_engine("klaR") %>%  #set naive bayes model'
  set_args(usekernel = FALSE) %>%  #set argument to false
  set_mode("classification") 
```

```{r}
titanic_wflow4 = workflow() %>% #create workflow
  add_model(nb_model) %>% #add model
  add_recipe(titanic_recipe) #add recipe 

nb_fit = fit(titanic_wflow4, titanic_train) #fit model to training data
```


### Question 9

Now you've fit four different models to your training data.

Use `predict()` and `bind_cols()` to generate predictions using each of these 4 models and your **training** data. Then use the *accuracy* metric to assess the performance of each of the four models.

```{r}
glm_predict = predict(glm_fit, new_data = titanic_train) %>% 
  bind_cols(titanic_train) #predict training data with glm
```

```{r}
lda_predict = predict(lda_fit, new_data = titanic_train) %>% 
  bind_cols(titanic_train) #predict training data with lda
```

```{r}
qda_predict = predict(qda_fit, new_data = titanic_train) %>% 
  bind_cols(titanic_train) #predict training data with qda
```

```{r}
nb_predict = predict(nb_fit, new_data = titanic_train) %>% 
  bind_cols(titanic_train) #predict training data with nb
```

Which model achieved the highest accuracy on the training data?

```{r}
titanic_metrics <- metric_set(accuracy) #create metric set 
```


```{r}
titanic_metrics(glm_predict, truth = survived, 
                estimate = .pred_class) #glm accuracy

titanic_metrics(lda_predict, truth = survived, 
                estimate = .pred_class) #lda accuracy

titanic_metrics(qda_predict, truth = survived, 
                estimate = .pred_class) #qda accuracy

titanic_metrics(nb_predict, truth = survived, 
                estimate = .pred_class) #nb accuracy
```


**Answer:** The quadratic discriminant analysis model had the highest accuracy.

### Question 10

Fit the model with the highest training accuracy to the **testing** data. Report the accuracy of the model on the **testing** data.

```{r}
test_predict = predict(qda_fit, new_data = titanic_test, type = "prob") %>%
  bind_cols(titanic_test) %>%  #get test data prediction
  mutate(pred_class = as.factor(case_when(
    .pred_Yes > .pred_No ~ "Yes",
    .pred_Yes < .pred_No ~ "No" #get currect labels
  )))

test_predict$pred_class = factor(test_predict$pred_class, levels = c("Yes", "No")) #reorder factor survived
```

```{r}
accuracy(data = test_predict, truth = survived, estimate = pred_class) #test accuracy
```

Again using the **testing** data, create a confusion matrix and visualize it. Plot an ROC curve and calculate the area under it (AUC).

```{r}
conf_mat(test_predict, survived, pred_class) #confusion matrix
```

```{r}
autoplot(roc_curve(data = test_predict, truth = survived, .pred_Yes))
```

```{r}
roc_auc(test_predict, survived, .pred_Yes)
```

How did the model perform? Compare its training and testing accuracies. If the values differ, why do you think this is so?

**Answer:** The model preformed relatively well, with accuracies over 80% on both the training and test set. Similar training and test values, as well as a high AUC indicate good model fit. 

### Required for 231 Students

In a binary classification problem, let $p$ represent the probability of class label $1$, which implies that $1 - p$ represents the probability of class label $0$. The *logistic function* (also called the "inverse logit") is the cumulative distribution function of the logistic distribution, which maps a real number *z* to the open interval $(0, 1)$.

### Question 11

Given that:

$$
p(z)=\frac{e^z}{1+e^z}
$$

Prove that the inverse of a logistic function is indeed the *logit* function:

$$
z(p)=ln\left(\frac{p}{1-p}\right)
$$

### Question 12

Assume that $z = \beta_0 + \beta_{1}x_{1}$ and $p = logistic(z)$. How do the odds of the outcome change if you increase $x_{1}$ by two? Demonstrate this.

Assume now that $\beta_1$ is negative. What value does $p$ approach as $x_{1}$ approaches $\infty$? What value does $p$ approach as $x_{1}$ approaches $-\infty$?